{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-26T14:54:34.244061Z",
          "iopub.status.busy": "2021-11-26T14:54:34.243235Z",
          "iopub.status.idle": "2021-11-26T14:54:40.17511Z",
          "shell.execute_reply": "2021-11-26T14:54:40.174395Z",
          "shell.execute_reply.started": "2021-11-26T14:54:34.243939Z"
        },
        "id": "Kad8mvnp85ML",
        "outputId": "34c262c7-485d-4bc3-b6a4-db1264ab6540"
      },
      "source": [
        "Authors: *Alessia Cotroneo - Alessandro Del Vitto- Michele Di Sabato*\n",
        "\n",
        "In this notebook we implement a pure (no fine tuning) Transfer Learning, using InceptionV3 architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU3jDhE61QPT",
        "outputId": "f4e0c68a-2838-4225-84d7-a78152492ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbIYeB0K1Qu2",
        "outputId": "1e3c4694-f776-410b-a00f-67e8feffc229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/ANN&DL First Challenge\n"
          ]
        }
      ],
      "source": [
        "%cd /gdrive/MyDrive/path/to/your/folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrfUyQSz1V0J"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uaj86Rb1c9v"
      },
      "outputs": [],
      "source": [
        "dataset_dir = 'training'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emo28n5m0h01"
      },
      "source": [
        "## Import libraries and set seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:27:55.002751Z",
          "iopub.status.busy": "2021-11-27T14:27:55.002448Z",
          "iopub.status.idle": "2021-11-27T14:27:55.025689Z",
          "shell.execute_reply": "2021-11-27T14:27:55.025033Z",
          "shell.execute_reply.started": "2021-11-27T14:27:55.002722Z"
        },
        "id": "ANZZO6dL0h02",
        "outputId": "0adb2f1c-6304-4fab-f9d2-10bdfd8d9780",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "# import the relevant libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "print(tf.__version__)\n",
        "\n",
        "# Random seed for reproducibility\n",
        "seed = 123\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:27:59.953047Z",
          "iopub.status.busy": "2021-11-27T14:27:59.952338Z",
          "iopub.status.idle": "2021-11-27T14:27:59.960302Z",
          "shell.execute_reply": "2021-11-27T14:27:59.959607Z",
          "shell.execute_reply.started": "2021-11-27T14:27:59.952994Z"
        },
        "id": "5vrdMYePx3Xu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_next_batch(generator):\n",
        "    # get new batch of data and plot it\n",
        "  batch = next(generator)\n",
        "\n",
        "  image = batch[0]\n",
        "  target = batch[1]\n",
        "\n",
        "  print(\"(Input) image shape:\", image.shape)\n",
        "  print(\"Target shape:\",target.shape)\n",
        "\n",
        "  # Visualize only the first sample\n",
        "  image = image[0]\n",
        "  target = target[0]\n",
        "  target_idx = np.argmax(target)\n",
        "  print()\n",
        "  print(\"Categorical label:\", target)\n",
        "  print(\"Label:\", target_idx)\n",
        "  print(\"Class name:\", labels[target_idx])\n",
        "  \n",
        "  # plot the color intensity of the \"central\" pixel to check if preprocessing worked\n",
        "  print(\"Middle Pixel Value R: \", image[125][125][0])\n",
        "  print(\"Middle Pixel Value G: \", image[125][125][1])\n",
        "  print(\"Middle Pixel Value B: \", image[125][125][2])\n",
        "  fig = plt.figure(figsize=(6, 4))\n",
        "  plt.imshow(np.uint8(image))\n",
        "\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcuiYYqvjx5H"
      },
      "source": [
        "## Model's hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:29:20.240329Z",
          "iopub.status.busy": "2021-11-27T14:29:20.239661Z",
          "iopub.status.idle": "2021-11-27T14:29:20.244145Z",
          "shell.execute_reply": "2021-11-27T14:29:20.243391Z",
          "shell.execute_reply.started": "2021-11-27T14:29:20.240290Z"
        },
        "id": "4d-CS1j8cVrQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "target_size = 256\n",
        "input_shape = (target_size, target_size, 3)\n",
        "epochs = 50\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWtnJ1QHef8-"
      },
      "source": [
        "## Function that creates checkpoints and implements EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:29:22.629190Z",
          "iopub.status.busy": "2021-11-27T14:29:22.628537Z",
          "iopub.status.idle": "2021-11-27T14:29:22.638246Z",
          "shell.execute_reply": "2021-11-27T14:29:22.637372Z",
          "shell.execute_reply.started": "2021-11-27T14:29:22.629156Z"
        },
        "id": "-37ig0Yqdejl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Utility function to create folders and callbacks for training\n",
        "from datetime import datetime\n",
        "\n",
        "def create_folders_and_callbacks(model_name,transfer_learning=False):\n",
        "    \n",
        "  # creates folders and callbacks, collected into a list, since the argument \"callbacks\" in the function .fit() requires a list\n",
        "  \n",
        "  #  if transfer_learning = True this function creates a folder called \"CNN_TL\", otherwise the name is just \"CNN\"\n",
        "\n",
        "  if transfer_learning:\n",
        "    exps_dir = os.path.join('CNN_TL')\n",
        "  else:\n",
        "    exps_dir = os.path.join('CNN')\n",
        "  if not os.path.exists(exps_dir):\n",
        "      os.makedirs(exps_dir) # creates and SETS the directory if not found\n",
        "\n",
        "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "  if not os.path.exists(exp_dir):\n",
        "      os.makedirs(exp_dir)\n",
        "      \n",
        "  callbacks = []\n",
        "\n",
        "  # Model checkpoint\n",
        "  # ----------------\n",
        "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "      os.makedirs(ckpt_dir)\n",
        "\n",
        "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n",
        "                                                     save_weights_only=False, # True to save only weights\n",
        "                                                     save_best_only=False) # True to save only the best epoch \n",
        "  callbacks.append(ckpt_callback)\n",
        "\n",
        "  # Visualize Learning on Tensorboard\n",
        "  # ---------------------------------\n",
        "  tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "  if not os.path.exists(tb_dir):\n",
        "      os.makedirs(tb_dir)\n",
        "      \n",
        "  # By default shows losses and metrics for both training and validation\n",
        "  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
        "                                               profile_batch=0,\n",
        "                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n",
        "                                                                  # histogram_freq = 5 means that i want to save 1 in 5 epochs (e.g. 1, 6, 6+5, ...)\n",
        "                                                                  # histogram_freq = 1 means that i want to save 1 in 5 epochs (e.g. 1, 2, 3, ...)\n",
        "  callbacks.append(tb_callback)\n",
        "\n",
        "  # Early Stopping\n",
        "  # --------------\n",
        "  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode = \"max\", patience=10, restore_best_weights=True)\n",
        "  #es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode = \"min\", patience=30, restore_best_weights=True)\n",
        "  callbacks.append(es_callback)\n",
        "\n",
        "  return callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znJZsq_Zrdwx"
      },
      "source": [
        "## Training and Validation splitting in the context of Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:29:45.824426Z",
          "iopub.status.busy": "2021-11-27T14:29:45.824005Z",
          "iopub.status.idle": "2021-11-27T14:29:46.372920Z",
          "shell.execute_reply": "2021-11-27T14:29:46.372143Z",
          "shell.execute_reply.started": "2021-11-27T14:29:45.824397Z"
        },
        "id": "VsKDmrNrsH2m",
        "outputId": "6cfb1051-e8ea-41a6-bfd1-d9fcf2c5cc9e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14189 images belonging to 14 classes.\n",
            "Found 3539 images belonging to 14 classes.\n"
          ]
        }
      ],
      "source": [
        "labels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']\n",
        "# parameters for the ImageDataGenerator function (preprocessing)\n",
        "reshape_mean = False\n",
        "reshape_sd   = False\n",
        "use_data_augmentation = True\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "if use_data_augmentation:\n",
        "  train_data_gen = ImageDataGenerator(validation_split=0.2, samplewise_center = reshape_mean, samplewise_std_normalization = reshape_sd,\n",
        "                                      rotation_range = 30,\n",
        "                                      height_shift_range = 50,\n",
        "                                      width_shift_range = 50,\n",
        "                                      zoom_range = 0.3,\n",
        "                                      horizontal_flip = True,\n",
        "                                      vertical_flip = True,\n",
        "                                      fill_mode = \"constant\",\n",
        "                                      rescale = 1/255.,\n",
        "                                      preprocessing_function = preprocess_input)\n",
        "else:\n",
        "  train_data_gen = ImageDataGenerator(validation_split=0.2,\n",
        "                                      rescale = 1/255.,\n",
        "                                      preprocessing_function = preprocess_input)  \n",
        "target_size = 256\n",
        "train_gen = train_data_gen.flow_from_directory(directory=dataset_dir,\n",
        "                                               target_size=(target_size,target_size),\n",
        "                                               color_mode='rgb',\n",
        "                                               classes=None,\n",
        "                                               class_mode='categorical',\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               seed=seed,\n",
        "                                               subset=\"training\")\n",
        "val_gen = train_data_gen.flow_from_directory(directory=dataset_dir,\n",
        "                                               target_size=(target_size,target_size),\n",
        "                                               color_mode='rgb',\n",
        "                                               classes=None,\n",
        "                                               class_mode='categorical',\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=False,\n",
        "                                               seed=seed,\n",
        "                                               subset=\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:29:58.499706Z",
          "iopub.status.busy": "2021-11-27T14:29:58.499308Z",
          "iopub.status.idle": "2021-11-27T14:29:58.502625Z",
          "shell.execute_reply": "2021-11-27T14:29:58.502096Z",
          "shell.execute_reply.started": "2021-11-27T14:29:58.499678Z"
        },
        "id": "jz8BRV_R0h1C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if False: # used to check is ImageDataGenerator was successful\n",
        "  _ = get_next_batch(train_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wta_YWPX3vU_"
      },
      "source": [
        "### load InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:03.841949Z",
          "iopub.status.busy": "2021-11-27T14:30:03.841534Z",
          "iopub.status.idle": "2021-11-27T14:30:06.331940Z",
          "shell.execute_reply": "2021-11-27T14:30:06.331176Z",
          "shell.execute_reply.started": "2021-11-27T14:30:03.841919Z"
        },
        "id": "8ayy3digrdJ-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "supernet = tfk.applications.InceptionV3(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(299,299,3) # the input image of InceptionV3 is a 299x299 picture\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7oqOdz35To"
      },
      "source": [
        "### design the architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:17.270329Z",
          "iopub.status.busy": "2021-11-27T14:30:17.269816Z",
          "iopub.status.idle": "2021-11-27T14:30:17.941538Z",
          "shell.execute_reply": "2021-11-27T14:30:17.940925Z",
          "shell.execute_reply.started": "2021-11-27T14:30:17.270293Z"
        },
        "id": "B6BWzWFmtH5Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "supernet.trainable = False # no fine tuning\n",
        "new_size = 299\n",
        "input_size = 256\n",
        "\n",
        "inputs = tfk.Input(shape=(input_size,input_size,3))\n",
        "\n",
        "x = tfkl.Resizing(new_size, new_size, interpolation=\"bicubic\")(inputs)\n",
        "\n",
        "x = supernet(x)\n",
        "\n",
        "x = tfkl.Flatten(name='Flattening')(x)\n",
        "\n",
        "x = tfkl.Dense(\n",
        "        128, \n",
        "        activation='relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)\n",
        "\n",
        "x = tfkl.Dropout(0.2, seed=seed)(x)\n",
        "\n",
        "x = tfkl.Dense(\n",
        "        128, \n",
        "        activation='relu',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)\n",
        "\n",
        "x = tfkl.Dropout(0.2, seed=seed)(x)\n",
        "\n",
        "outputs = tfkl.Dense(\n",
        "        14, \n",
        "        activation='softmax',\n",
        "        kernel_initializer = tfk.initializers.GlorotUniform(seed),\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)\n",
        "\n",
        "tf_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "# learning rate = 1e-4 since we are using Transfer Learning\n",
        "tf_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyZqN8gN4ADH"
      },
      "source": [
        "### summarize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:23.449099Z",
          "iopub.status.busy": "2021-11-27T14:30:23.448409Z",
          "iopub.status.idle": "2021-11-27T14:30:23.470692Z",
          "shell.execute_reply": "2021-11-27T14:30:23.469593Z",
          "shell.execute_reply.started": "2021-11-27T14:30:23.449056Z"
        },
        "id": "9CzTHKlXuLe_",
        "outputId": "4de68a46-210b-4b5a-803f-a029e3c09b0b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 299, 299, 3)       0         \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
            "                                                                 \n",
            " Flattening (Flatten)        (None, 131072)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16777344  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 14)                1806      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,598,446\n",
            "Trainable params: 16,795,662\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnMSxkej0h1F"
      },
      "source": [
        "#### **DISCLAIMER**: We have written the following code in a way that it can be generalized to Fine Tuning, even though we don't do it in this Notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:29.616210Z",
          "iopub.status.busy": "2021-11-27T14:30:29.615760Z",
          "iopub.status.idle": "2021-11-27T14:30:29.639260Z",
          "shell.execute_reply": "2021-11-27T14:30:29.638557Z",
          "shell.execute_reply.started": "2021-11-27T14:30:29.616163Z"
        },
        "id": "DcAcGevR0h1F",
        "outputId": "066f60f7-f986-470f-fab4-67f976520976",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 299, 299, 3)       0         \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
            "                                                                 \n",
            " Flattening (Flatten)        (None, 131072)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16777344  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 14)                1806      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,598,446\n",
            "Trainable params: 16,795,662\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "ft_model = tf_model\n",
        "ft_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwAaUA943cs4"
      },
      "source": [
        "## choose the layers for the fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:45.442511Z",
          "iopub.status.busy": "2021-11-27T14:30:45.442073Z",
          "iopub.status.idle": "2021-11-27T14:30:45.598457Z",
          "shell.execute_reply": "2021-11-27T14:30:45.597611Z",
          "shell.execute_reply.started": "2021-11-27T14:30:45.442479Z"
        },
        "id": "1QM1VU4H0h1F",
        "outputId": "78c7010c-7916-450d-97a4-8a01d72c6182",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 input_1 False\n",
            "1 conv2d False\n",
            "2 batch_normalization False\n",
            "3 activation False\n",
            "4 conv2d_1 False\n",
            "5 batch_normalization_1 False\n",
            "6 activation_1 False\n",
            "7 conv2d_2 False\n",
            "8 batch_normalization_2 False\n",
            "9 activation_2 False\n",
            "10 max_pooling2d False\n",
            "11 conv2d_3 False\n",
            "12 batch_normalization_3 False\n",
            "13 activation_3 False\n",
            "14 conv2d_4 False\n",
            "15 batch_normalization_4 False\n",
            "16 activation_4 False\n",
            "17 max_pooling2d_1 False\n",
            "18 conv2d_8 False\n",
            "19 batch_normalization_8 False\n",
            "20 activation_8 False\n",
            "21 conv2d_6 False\n",
            "22 conv2d_9 False\n",
            "23 batch_normalization_6 False\n",
            "24 batch_normalization_9 False\n",
            "25 activation_6 False\n",
            "26 activation_9 False\n",
            "27 average_pooling2d False\n",
            "28 conv2d_5 False\n",
            "29 conv2d_7 False\n",
            "30 conv2d_10 False\n",
            "31 conv2d_11 False\n",
            "32 batch_normalization_5 False\n",
            "33 batch_normalization_7 False\n",
            "34 batch_normalization_10 False\n",
            "35 batch_normalization_11 False\n",
            "36 activation_5 False\n",
            "37 activation_7 False\n",
            "38 activation_10 False\n",
            "39 activation_11 False\n",
            "40 mixed0 False\n",
            "41 conv2d_15 False\n",
            "42 batch_normalization_15 False\n",
            "43 activation_15 False\n",
            "44 conv2d_13 False\n",
            "45 conv2d_16 False\n",
            "46 batch_normalization_13 False\n",
            "47 batch_normalization_16 False\n",
            "48 activation_13 False\n",
            "49 activation_16 False\n",
            "50 average_pooling2d_1 False\n",
            "51 conv2d_12 False\n",
            "52 conv2d_14 False\n",
            "53 conv2d_17 False\n",
            "54 conv2d_18 False\n",
            "55 batch_normalization_12 False\n",
            "56 batch_normalization_14 False\n",
            "57 batch_normalization_17 False\n",
            "58 batch_normalization_18 False\n",
            "59 activation_12 False\n",
            "60 activation_14 False\n",
            "61 activation_17 False\n",
            "62 activation_18 False\n",
            "63 mixed1 False\n",
            "64 conv2d_22 False\n",
            "65 batch_normalization_22 False\n",
            "66 activation_22 False\n",
            "67 conv2d_20 False\n",
            "68 conv2d_23 False\n",
            "69 batch_normalization_20 False\n",
            "70 batch_normalization_23 False\n",
            "71 activation_20 False\n",
            "72 activation_23 False\n",
            "73 average_pooling2d_2 False\n",
            "74 conv2d_19 False\n",
            "75 conv2d_21 False\n",
            "76 conv2d_24 False\n",
            "77 conv2d_25 False\n",
            "78 batch_normalization_19 False\n",
            "79 batch_normalization_21 False\n",
            "80 batch_normalization_24 False\n",
            "81 batch_normalization_25 False\n",
            "82 activation_19 False\n",
            "83 activation_21 False\n",
            "84 activation_24 False\n",
            "85 activation_25 False\n",
            "86 mixed2 False\n",
            "87 conv2d_27 False\n",
            "88 batch_normalization_27 False\n",
            "89 activation_27 False\n",
            "90 conv2d_28 False\n",
            "91 batch_normalization_28 False\n",
            "92 activation_28 False\n",
            "93 conv2d_26 False\n",
            "94 conv2d_29 False\n",
            "95 batch_normalization_26 False\n",
            "96 batch_normalization_29 False\n",
            "97 activation_26 False\n",
            "98 activation_29 False\n",
            "99 max_pooling2d_2 False\n",
            "100 mixed3 False\n",
            "101 conv2d_34 False\n",
            "102 batch_normalization_34 False\n",
            "103 activation_34 False\n",
            "104 conv2d_35 False\n",
            "105 batch_normalization_35 False\n",
            "106 activation_35 False\n",
            "107 conv2d_31 False\n",
            "108 conv2d_36 False\n",
            "109 batch_normalization_31 False\n",
            "110 batch_normalization_36 False\n",
            "111 activation_31 False\n",
            "112 activation_36 False\n",
            "113 conv2d_32 False\n",
            "114 conv2d_37 False\n",
            "115 batch_normalization_32 False\n",
            "116 batch_normalization_37 False\n",
            "117 activation_32 False\n",
            "118 activation_37 False\n",
            "119 average_pooling2d_3 False\n",
            "120 conv2d_30 False\n",
            "121 conv2d_33 False\n",
            "122 conv2d_38 False\n",
            "123 conv2d_39 False\n",
            "124 batch_normalization_30 False\n",
            "125 batch_normalization_33 False\n",
            "126 batch_normalization_38 False\n",
            "127 batch_normalization_39 False\n",
            "128 activation_30 False\n",
            "129 activation_33 False\n",
            "130 activation_38 False\n",
            "131 activation_39 False\n",
            "132 mixed4 False\n",
            "133 conv2d_44 False\n",
            "134 batch_normalization_44 False\n",
            "135 activation_44 False\n",
            "136 conv2d_45 False\n",
            "137 batch_normalization_45 False\n",
            "138 activation_45 False\n",
            "139 conv2d_41 False\n",
            "140 conv2d_46 False\n",
            "141 batch_normalization_41 False\n",
            "142 batch_normalization_46 False\n",
            "143 activation_41 False\n",
            "144 activation_46 False\n",
            "145 conv2d_42 False\n",
            "146 conv2d_47 False\n",
            "147 batch_normalization_42 False\n",
            "148 batch_normalization_47 False\n",
            "149 activation_42 False\n",
            "150 activation_47 False\n",
            "151 average_pooling2d_4 False\n",
            "152 conv2d_40 False\n",
            "153 conv2d_43 False\n",
            "154 conv2d_48 False\n",
            "155 conv2d_49 False\n",
            "156 batch_normalization_40 False\n",
            "157 batch_normalization_43 False\n",
            "158 batch_normalization_48 False\n",
            "159 batch_normalization_49 False\n",
            "160 activation_40 False\n",
            "161 activation_43 False\n",
            "162 activation_48 False\n",
            "163 activation_49 False\n",
            "164 mixed5 False\n",
            "165 conv2d_54 False\n",
            "166 batch_normalization_54 False\n",
            "167 activation_54 False\n",
            "168 conv2d_55 False\n",
            "169 batch_normalization_55 False\n",
            "170 activation_55 False\n",
            "171 conv2d_51 False\n",
            "172 conv2d_56 False\n",
            "173 batch_normalization_51 False\n",
            "174 batch_normalization_56 False\n",
            "175 activation_51 False\n",
            "176 activation_56 False\n",
            "177 conv2d_52 False\n",
            "178 conv2d_57 False\n",
            "179 batch_normalization_52 False\n",
            "180 batch_normalization_57 False\n",
            "181 activation_52 False\n",
            "182 activation_57 False\n",
            "183 average_pooling2d_5 False\n",
            "184 conv2d_50 False\n",
            "185 conv2d_53 False\n",
            "186 conv2d_58 False\n",
            "187 conv2d_59 False\n",
            "188 batch_normalization_50 False\n",
            "189 batch_normalization_53 False\n",
            "190 batch_normalization_58 False\n",
            "191 batch_normalization_59 False\n",
            "192 activation_50 False\n",
            "193 activation_53 False\n",
            "194 activation_58 False\n",
            "195 activation_59 False\n",
            "196 mixed6 False\n",
            "197 conv2d_64 False\n",
            "198 batch_normalization_64 False\n",
            "199 activation_64 False\n",
            "200 conv2d_65 False\n",
            "201 batch_normalization_65 False\n",
            "202 activation_65 False\n",
            "203 conv2d_61 False\n",
            "204 conv2d_66 False\n",
            "205 batch_normalization_61 False\n",
            "206 batch_normalization_66 False\n",
            "207 activation_61 False\n",
            "208 activation_66 False\n",
            "209 conv2d_62 False\n",
            "210 conv2d_67 False\n",
            "211 batch_normalization_62 False\n",
            "212 batch_normalization_67 False\n",
            "213 activation_62 False\n",
            "214 activation_67 False\n",
            "215 average_pooling2d_6 False\n",
            "216 conv2d_60 False\n",
            "217 conv2d_63 False\n",
            "218 conv2d_68 False\n",
            "219 conv2d_69 False\n",
            "220 batch_normalization_60 False\n",
            "221 batch_normalization_63 False\n",
            "222 batch_normalization_68 False\n",
            "223 batch_normalization_69 False\n",
            "224 activation_60 False\n",
            "225 activation_63 False\n",
            "226 activation_68 False\n",
            "227 activation_69 False\n",
            "228 mixed7 False\n",
            "229 conv2d_72 False\n",
            "230 batch_normalization_72 False\n",
            "231 activation_72 False\n",
            "232 conv2d_73 False\n",
            "233 batch_normalization_73 False\n",
            "234 activation_73 False\n",
            "235 conv2d_70 False\n",
            "236 conv2d_74 False\n",
            "237 batch_normalization_70 False\n",
            "238 batch_normalization_74 False\n",
            "239 activation_70 False\n",
            "240 activation_74 False\n",
            "241 conv2d_71 False\n",
            "242 conv2d_75 False\n",
            "243 batch_normalization_71 False\n",
            "244 batch_normalization_75 False\n",
            "245 activation_71 False\n",
            "246 activation_75 False\n",
            "247 max_pooling2d_3 False\n",
            "248 mixed8 False\n",
            "249 conv2d_80 False\n",
            "250 batch_normalization_80 False\n",
            "251 activation_80 False\n",
            "252 conv2d_77 False\n",
            "253 conv2d_81 False\n",
            "254 batch_normalization_77 False\n",
            "255 batch_normalization_81 False\n",
            "256 activation_77 False\n",
            "257 activation_81 False\n",
            "258 conv2d_78 False\n",
            "259 conv2d_79 False\n",
            "260 conv2d_82 False\n",
            "261 conv2d_83 False\n",
            "262 average_pooling2d_7 False\n",
            "263 conv2d_76 False\n",
            "264 batch_normalization_78 False\n",
            "265 batch_normalization_79 False\n",
            "266 batch_normalization_82 False\n",
            "267 batch_normalization_83 False\n",
            "268 conv2d_84 False\n",
            "269 batch_normalization_76 False\n",
            "270 activation_78 False\n",
            "271 activation_79 False\n",
            "272 activation_82 False\n",
            "273 activation_83 False\n",
            "274 batch_normalization_84 False\n",
            "275 activation_76 False\n",
            "276 mixed9_0 False\n",
            "277 concatenate False\n",
            "278 activation_84 False\n",
            "279 mixed9 False\n",
            "280 conv2d_89 False\n",
            "281 batch_normalization_89 False\n",
            "282 activation_89 False\n",
            "283 conv2d_86 False\n",
            "284 conv2d_90 False\n",
            "285 batch_normalization_86 False\n",
            "286 batch_normalization_90 False\n",
            "287 activation_86 False\n",
            "288 activation_90 False\n",
            "289 conv2d_87 False\n",
            "290 conv2d_88 False\n",
            "291 conv2d_91 False\n",
            "292 conv2d_92 False\n",
            "293 average_pooling2d_8 False\n",
            "294 conv2d_85 False\n",
            "295 batch_normalization_87 False\n",
            "296 batch_normalization_88 False\n",
            "297 batch_normalization_91 False\n",
            "298 batch_normalization_92 False\n",
            "299 conv2d_93 False\n",
            "300 batch_normalization_85 False\n",
            "301 activation_87 False\n",
            "302 activation_88 False\n",
            "303 activation_91 False\n",
            "304 activation_92 False\n",
            "305 batch_normalization_93 False\n",
            "306 activation_85 False\n",
            "307 mixed9_1 False\n",
            "308 concatenate_1 False\n",
            "309 activation_93 False\n",
            "310 mixed10 False\n"
          ]
        }
      ],
      "source": [
        "ft_model.get_layer('inception_v3').trainable = True\n",
        "# InceptionV3 has 311 layers: if we want to fine tune it, we can change the following parameter to tune a specific number of layers\n",
        "freeze_up_to_this_index = 311\n",
        "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers[:freeze_up_to_this_index]):\n",
        "  layer.trainable=False\n",
        "# print to check everythin is fine\n",
        "for i, layer in enumerate(ft_model.get_layer('inception_v3').layers):\n",
        "   print(i, layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:53.154276Z",
          "iopub.status.busy": "2021-11-27T14:30:53.153977Z",
          "iopub.status.idle": "2021-11-27T14:30:53.176665Z",
          "shell.execute_reply": "2021-11-27T14:30:53.175842Z",
          "shell.execute_reply.started": "2021-11-27T14:30:53.154243Z"
        },
        "id": "MtqlrWec0h1G",
        "outputId": "0a31a7b5-2597-431a-835b-51f3b52cbce8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 299, 299, 3)       0         \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
            "                                                                 \n",
            " Flattening (Flatten)        (None, 131072)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16777344  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 14)                1806      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,598,446\n",
            "Trainable params: 16,795,662\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "ft_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-27T14:30:58.457467Z",
          "iopub.status.busy": "2021-11-27T14:30:58.457215Z",
          "iopub.status.idle": "2021-11-27T14:30:58.470020Z",
          "shell.execute_reply": "2021-11-27T14:30:58.469346Z",
          "shell.execute_reply.started": "2021-11-27T14:30:58.457441Z"
        },
        "id": "vURTCFvK0h1G",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmU5pxRl4LAw"
      },
      "source": [
        "## fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "execution": {
          "iopub.execute_input": "2021-11-27T14:25:06.880521Z",
          "iopub.status.busy": "2021-11-27T14:25:06.880247Z",
          "iopub.status.idle": "2021-11-27T14:25:17.845425Z",
          "shell.execute_reply": "2021-11-27T14:25:17.844604Z",
          "shell.execute_reply.started": "2021-11-27T14:25:06.880493Z"
        },
        "id": "nv_r78Eb0h1G",
        "outputId": "faebc31b-1d9c-4df1-90ce-addc9e73ad9c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "FT_InceptionV3_callbacks = create_folders_and_callbacks(model_name='model_FT', transfer_learning = True)\n",
        "ft_history = ft_model.fit(\n",
        "    x = train_gen,\n",
        "    batch_size = 16,\n",
        "    epochs = 50,\n",
        "    validation_data = val_gen,\n",
        "    callbacks = FT_InceptionV3_callbacks\n",
        ").history"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TransferLearning_InceptionV3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "0d0d1bce3d99a369e1b3692bad887850a832ed7c7f63d0e0d78616c3419ee9db"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
